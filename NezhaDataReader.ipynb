{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f6137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSVçš„é˜…è¯»å’Œå¤„ç†\n",
    "# å¯¼å…¥å„ç§å„æ ·çš„åŒ…ï¼š\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "# è¿™æ˜¯ä¸€ä¸ªæ•°æ®è¯»å–çš„è„šæœ¬ï¼Œå¦‚æœä½ éœ€è¦ä½¿ç”¨ï¼Œè¯·åŠ¡å¿…è‡ªå·±è°ƒæ•´æ–‡ä»¶åœ°å€ï¼ï¼ï¼ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ced1b321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ–‡ä»¶ç¼–ç ä¸ºï¼šGB2312\n"
     ]
    }
   ],
   "source": [
    "with open('C:\\\\Users\\\\é¡¾å­çª\\\\Desktop\\\\ä¸Šçº½å¤§æ–‡ä»¶\\\\2025 SPRING\\\\Nezha2.csv', 'rb') as f:\n",
    "    import chardet\n",
    "    result = chardet.detect(f.read(10000))\n",
    "    encoding = result['encoding']\n",
    "    print(f\"æ–‡ä»¶ç¼–ç ä¸ºï¼š{encoding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b90b3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['comment_id', 'username', 'user_id', 'score', 'content', 'time', 'city',\n",
      "       'gender', 'approve_count', 'reply_count', 'user_level', 'vip_type',\n",
      "       'is_vip', 'avatar_url', 'tags', 'is_purchase', 'is_hot'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\é¡¾å­çª\\\\Desktop\\\\ä¸Šçº½å¤§æ–‡ä»¶\\\\2025 SPRING\\\\Nezha2.csv', encoding='gb18030', low_memory=False)\n",
    "#æ‰“å°columns:\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5166285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "é‡æ„åçš„æ•°æ®ï¼š\n",
      "   comment_id  score   content             time city gender\n",
      "0  1211665559    5.0   äºŒåˆ·ï¼ä¾æ—§ç²¾å½©  2025/4/21 21:16   ä½›å±±     æœªçŸ¥\n",
      "1  1211652698    5.0  æ”¯æŒä¸€ä¸‹å›½äº§åŠ¨æ¼«  2025/4/21 21:15   å»¶è¾¹      1\n",
      "2  1211623835    5.0   å‰§æƒ…æç¬‘åˆç´§å¼   2025/4/21 21:13   æ­¦æ±‰     æœªçŸ¥\n",
      "3  1211642772    3.5  è¿˜è¡Œå§ï¼Œç‰¹æ•ˆä¸é”™  2025/4/21 21:08   æ·±åœ³     æœªçŸ¥\n",
      "4  1211659669    4.5    å€¼å¾—çœ‹çš„ä¸€éƒ¨  2025/4/21 20:59   æµ·ä¸œ     æœªçŸ¥\n",
      "\n",
      "âœ… é‡æ„åçš„ CSV å·²ä¿å­˜ä¸º 'Nezha2_dropped_unnecessary_cols.csv'\n"
     ]
    }
   ],
   "source": [
    "#é‡æ„CSVï¼Œæˆ‘ä»¬å°†ä¼šä¸¢å»ä¸€äº›åˆ—ï¼š'comment_id', 'username', 'user_id', 'approve_count', 'reply_count', 'user_level', 'vip_type', 'is_vip', 'avatar_url', 'tags', 'is_purchase', 'is_hot'\n",
    "\n",
    "# è¦ä¸¢å¼ƒçš„åˆ—\n",
    "columns_to_drop = [\n",
    "    'username', 'user_id', 'approve_count', 'reply_count',\n",
    "    'user_level', 'vip_type', 'is_vip', 'avatar_url', 'tags',\n",
    "    'is_purchase', 'is_hot'\n",
    "]\n",
    "\n",
    "# æ£€æŸ¥è¿™äº›åˆ—æ˜¯å¦éƒ½å­˜åœ¨äºå½“å‰ DataFrame ä¸­ï¼ˆé˜²æ­¢æŠ¥é”™ï¼‰\n",
    "columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "\n",
    "# ä¸¢æ‰ä¸éœ€è¦çš„åˆ—\n",
    "df_restructured = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# æŸ¥çœ‹å‰å‡ è¡Œï¼Œç¡®è®¤ç»“æ„\n",
    "print(\"\\né‡æ„åçš„æ•°æ®ï¼š\")\n",
    "print(df_restructured.head())\n",
    "\n",
    "# å¯é€‰æ‹©ä¿å­˜ä¸ºæ–°æ–‡ä»¶ï¼ˆä»¥é˜²åŸæ–‡ä»¶è¢«è¦†ç›–ï¼‰\n",
    "df_restructured.to_csv('C:\\\\Users\\\\é¡¾å­çª\\\\Desktop\\\\ä¸Šçº½å¤§æ–‡ä»¶\\\\2025 SPRING\\\\Nezha2_dropped_unnecessary_cols.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"\\nâœ… é‡æ„åçš„ CSV å·²ä¿å­˜ä¸º 'Nezha2_dropped_unnecessary_cols.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "599b5763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['comment_id', 'score', 'content', 'time', 'city', 'gender'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åœ¨æ–°æ•°æ®é›†ä¸Šæ“ä½œï¼š\n",
    "df = pd.read_csv('C:\\\\Users\\\\é¡¾å­çª\\\\Desktop\\\\ä¸Šçº½å¤§æ–‡ä»¶\\\\2025 SPRING\\\\Nezha2_dropped_unnecessary_cols.csv', encoding='utf-8-sig', low_memory=False)\n",
    "#æ‰“å°columns:\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f86059f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨åˆ†ææƒ…æ„Ÿ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1326/1326 [00:11<00:00, 115.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    content  sentiment\n",
      "0   äºŒåˆ·ï¼ä¾æ—§ç²¾å½©   0.797696\n",
      "1  æ”¯æŒä¸€ä¸‹å›½äº§åŠ¨æ¼«   0.482468\n",
      "2   å‰§æƒ…æç¬‘åˆç´§å¼    0.633187\n",
      "3  è¿˜è¡Œå§ï¼Œç‰¹æ•ˆä¸é”™   0.947526\n",
      "4    å€¼å¾—çœ‹çš„ä¸€éƒ¨   0.852965\n",
      "\n",
      "âœ… æƒ…æ„Ÿåˆ†æå®Œæˆå¹¶ä¿å­˜ä¸ºï¼šC:\\Users\\é¡¾å­çª\\Desktop\\ä¸Šçº½å¤§æ–‡ä»¶\\2025 SPRING\\Nezha2_now_with_sentiment.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#æƒ…æ„Ÿåˆ†æï¼š\n",
    "from snownlp import SnowNLP\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"æ­£åœ¨åˆ†ææƒ…æ„Ÿ\")\n",
    "# æ£€æŸ¥æ˜¯å¦æœ‰ç©ºå€¼ï¼Œå¹¶å¡«å……ä¸º None\n",
    "df['content'] = df['content'].replace('', np.nan)\n",
    "# ä½¿ç”¨ SnowNLP è¿›è¡Œæƒ…æ„Ÿåˆ†æ\n",
    "df['sentiment'] = df['content'].progress_apply(lambda x: SnowNLP(str(x)).sentiments if pd.notnull(x) else None)\n",
    "\n",
    "# æ˜¾ç¤ºå‰å‡ è¡Œæ£€æŸ¥ç»“æœ\n",
    "print(df[['content', 'sentiment']].head())\n",
    "\n",
    "# ä¿å­˜å¸¦æƒ…æ„Ÿåˆ†æç»“æœçš„æ–° CSV\n",
    "output_path = 'C:\\\\Users\\\\é¡¾å­çª\\\\Desktop\\\\ä¸Šçº½å¤§æ–‡ä»¶\\\\2025 SPRING\\\\Nezha2_now_with_sentiment.csv'\n",
    "df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "print(f\"\\nâœ… æƒ…æ„Ÿåˆ†æå®Œæˆå¹¶ä¿å­˜ä¸ºï¼š{output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99056271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a840ac5e670404cadc330f8fdb4e400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\é¡¾å­çª\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\é¡¾å­çª\\.cache\\huggingface\\hub\\models--IDEA-CCNL--Erlangshen-Roberta-110M-Sentiment. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d80cf5ffa84932ae77ae8277ec1fa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/785 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a47e04a762cc418ba9178c35cc111167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/409M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æƒ…æ„Ÿå€¾å‘: tensor([[0.0087, 0.9913]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414faacc20b0436d9b7b6de717046e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/409M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#å¥½å§ï¼Œè¿™ä¸ªä¸œè¥¿çœ‹èµ·æ¥ä¸æ˜¯ç‰¹åˆ«çš„å‡†ï¼Œæˆ‘ä»¬æ¢ä¸€ä¸ªå·¥å…·åŒ…ï¼š\n",
    "#GPTï¼š ä½¿ç”¨HuggingFace Transformers + ä¸­æ–‡ BERT æƒ…æ„Ÿæ¨¡å‹\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# è½½å…¥æ¨¡å‹ï¼ˆç¤ºä¾‹æ¨¡å‹åï¼Œå¯æ¢ï¼‰\n",
    "model_name = \"IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "text = \"äºŒåˆ·ï¼ä¾æ—§ç²¾å½©\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "outputs = model(**inputs)\n",
    "probs = torch.softmax(outputs.logits, dim=1)\n",
    "print(\"æƒ…æ„Ÿå€¾å‘:\", probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6439c499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Sentiment:   0%|          | 0/1326 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Analyzing Sentiment: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1326/1326 [01:48<00:00, 12.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    content  sentiment\n",
      "0   äºŒåˆ·ï¼ä¾æ—§ç²¾å½©     0.9913\n",
      "1  æ”¯æŒä¸€ä¸‹å›½äº§åŠ¨æ¼«     0.9101\n",
      "2   å‰§æƒ…æç¬‘åˆç´§å¼      0.9853\n",
      "3  è¿˜è¡Œå§ï¼Œç‰¹æ•ˆä¸é”™     0.9776\n",
      "4    å€¼å¾—çœ‹çš„ä¸€éƒ¨     0.9496\n",
      "åˆ†æå®Œæˆï¼Œæ–‡ä»¶å·²ä¿å­˜ä¸ºï¼š C:\\Users\\é¡¾å­çª\\Desktop\\ä¸Šçº½å¤§æ–‡ä»¶\\2025 SPRING\\Nezha2_sentiment_by_advancedNLPs.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# è¯»å–æ•°æ®\n",
    "df = pd.read_csv('C:\\\\Users\\\\é¡¾å­çª\\\\Desktop\\\\ä¸Šçº½å¤§æ–‡ä»¶\\\\2025 SPRING\\\\Nezha2_dropped_unnecessary_cols.csv', encoding='utf-8-sig', low_memory=False)\n",
    "\n",
    "# åŠ è½½æ¨¡å‹å’Œ tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment\")\n",
    "\n",
    "# æ”¾åˆ°GPUå¦‚æœæœ‰çš„è¯\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# æƒ…æ„Ÿåˆ†æå‡½æ•°\n",
    "def get_positive_score(text):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return 0.0\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "        positive_score = probs[0][1].item()  # æ­£é¢æƒ…ç»ªæ¦‚ç‡\n",
    "    return round(positive_score, 4)\n",
    "\n",
    "# ç”¨ tqdm å¤„ç†æ‰€æœ‰è¯„è®º\n",
    "tqdm.pandas(desc=\"Analyzing Sentiment\")\n",
    "df[\"sentiment\"] = df[\"content\"].progress_apply(get_positive_score)\n",
    "\n",
    "# æ˜¾ç¤ºå‰å‡ è¡Œæ£€æŸ¥ç»“æœ\n",
    "print(df[['content', 'sentiment']].head())\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "output_path = 'C:\\\\Users\\\\é¡¾å­çª\\\\Desktop\\\\ä¸Šçº½å¤§æ–‡ä»¶\\\\2025 SPRING\\\\Nezha2_sentiment_by_advancedNLPs.csv'\n",
    "df.to_csv(output_path, encoding='utf-8-sig', index=False)\n",
    "\n",
    "print(\"åˆ†æå®Œæˆï¼Œæ–‡ä»¶å·²ä¿å­˜ä¸ºï¼š\", output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c77e984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å“¦å¯¹ï¼Œæ¢äº†å…ˆè¿›çš„NLPå·¥å…·åŒ…ä¹‹åï¼Œå‡†ç¡®åº¦ä¸Šå‡äº†ä¸å°‘ã€‚\n",
    "\"\"\"\n",
    "snow NLPè¯´ï¼š\n",
    "1211618654\t1.5\tå‰§æƒ…é€»è¾‘ã€äººç‰©å¡‘é€ ä¸èƒ½ç»†æƒ³\t2025/4/21 10:44\tåŒ—äº¬\t1\t0.7763131252505957\n",
    "\n",
    "ä½†æ˜¯advanced NLPè¿™æ ·è¯´ï¼š\n",
    "1211618654\t1.5\tå‰§æƒ…é€»è¾‘ã€äººç‰©å¡‘é€ ä¸èƒ½ç»†æƒ³\t2025/4/21 10:44\tåŒ—äº¬\t1\t0.0027\n",
    "\"\"\"\n",
    "\n",
    "#è¿™å°±å¾ˆå¯¹äº†ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd4e2839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… åˆæ³•çœä»½åŠè¯„è®ºæ•°é‡ï¼š\n",
      "å¹¿ä¸œçœ: 154 æ¡è¯„è®º\n",
      "æ±Ÿè‹çœ: 144 æ¡è¯„è®º\n",
      "å±±ä¸œçœ: 142 æ¡è¯„è®º\n",
      "åŒ—äº¬å¸‚: 77 æ¡è¯„è®º\n",
      "æµ™æ±Ÿçœ: 77 æ¡è¯„è®º\n",
      "å¹¿è¥¿å£®æ—è‡ªæ²»åŒº: 64 æ¡è¯„è®º\n",
      "å±±è¥¿çœ: 58 æ¡è¯„è®º\n",
      "å››å·çœ: 52 æ¡è¯„è®º\n",
      "è¾½å®çœ: 48 æ¡è¯„è®º\n",
      "æ²³å—çœ: 45 æ¡è¯„è®º\n",
      "æ¹–åŒ—çœ: 39 æ¡è¯„è®º\n",
      "æ²³åŒ—çœ: 37 æ¡è¯„è®º\n",
      "é™•è¥¿çœ: 36 æ¡è¯„è®º\n",
      "å®‰å¾½çœ: 32 æ¡è¯„è®º\n",
      "æ¹–å—çœ: 30 æ¡è¯„è®º\n",
      "å†…è’™å¤è‡ªæ²»åŒº: 22 æ¡è¯„è®º\n",
      "äº‘å—çœ: 21 æ¡è¯„è®º\n",
      "ç¦å»ºçœ: 20 æ¡è¯„è®º\n",
      "è´µå·çœ: 19 æ¡è¯„è®º\n",
      "ä¸Šæµ·å¸‚: 18 æ¡è¯„è®º\n",
      "é‡åº†å¸‚: 17 æ¡è¯„è®º\n",
      "é»‘é¾™æ±Ÿçœ: 17 æ¡è¯„è®º\n",
      "æ–°ç–†ç»´å¾å°”è‡ªæ²»åŒº: 16 æ¡è¯„è®º\n",
      "ç”˜è‚ƒçœ: 14 æ¡è¯„è®º\n",
      "å‰æ—çœ: 11 æ¡è¯„è®º\n",
      "å¤©æ´¥å¸‚: 11 æ¡è¯„è®º\n",
      "å®å¤å›æ—è‡ªæ²»åŒº: 10 æ¡è¯„è®º\n",
      "æ±Ÿè¥¿çœ: 9 æ¡è¯„è®º\n",
      "æµ·å—çœ: 9 æ¡è¯„è®º\n",
      "é’æµ·çœ: 6 æ¡è¯„è®º\n",
      "è¥¿è—è‡ªæ²»åŒº: 2 æ¡è¯„è®º\n",
      "\n",
      "ğŸ‰ æ–‡ä»¶å·²ä¿å­˜ä¸º Nezha2_with_province.csv\n"
     ]
    }
   ],
   "source": [
    "#OK,æ¥ä¸‹æ¥ï¼ŒæŠŠæˆ‘ä»¬çš„åŸå¸‚è½¬å˜ä¸ºçœä»½ï¼š\n",
    "\n",
    "#å¯¼åŒ…ï¼š\n",
    "import pandas as pd\n",
    "import cpca\n",
    "from tqdm import tqdm\n",
    "\n",
    "# è¯»å–æ•°æ®\n",
    "df = pd.read_csv('C:\\\\Users\\\\é¡¾å­çª\\\\Desktop\\\\ä¸Šçº½å¤§æ–‡ä»¶\\\\2025 SPRING\\\\Nezha2_sentiment_by_advancedNLPs.csv', encoding='utf-8-sig', low_memory=False)\n",
    "\n",
    "# åˆå§‹åŒ– tqdm è¿›åº¦æ¡\n",
    "tqdm.pandas(desc=\"è§£æåŸå¸‚åˆ°çœä»½\")\n",
    "\n",
    "# ä½¿ç”¨ cpca æ—¶ï¼Œéœ€è¦å°†åŸå¸‚åˆ—è½¬æ¢ä¸º pandas Series ç±»å‹\n",
    "df['province'] = cpca.transform(df['city'].values)['çœ']\n",
    "\n",
    "# å¡«å……â€œæœªçŸ¥çœä»½â€\n",
    "df['province'] = df['province'].fillna('æœªçŸ¥çœä»½')\n",
    "\n",
    "# æ‰“å°åˆæ³•çœä»½åŠè¯„è®ºæ•°é‡ï¼ˆå»é™¤â€œæœªçŸ¥çœä»½â€ï¼‰\n",
    "province_counts = df[df['province'] != 'æœªçŸ¥çœä»½']['province'].value_counts()\n",
    "\n",
    "print(\"\\nâœ… åˆæ³•çœä»½åŠè¯„è®ºæ•°é‡ï¼š\")\n",
    "for province, count in province_counts.items():\n",
    "    print(f\"{province}: {count} æ¡è¯„è®º\")\n",
    "\n",
    "# ä¿å­˜å¸¦çœä»½çš„ CSV\n",
    "df.to_csv('C:\\\\Users\\\\é¡¾å­çª\\\\Desktop\\\\ä¸Šçº½å¤§æ–‡ä»¶\\\\2025 SPRING\\\\Nezha2_with_province.csv', encoding='utf-8-sig', index=False)\n",
    "\n",
    "print(\"\\nğŸ‰ æ–‡ä»¶å·²ä¿å­˜ä¸º Nezha2_with_province.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d97a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     çœ     å¸‚     åŒº åœ°å€  adcode\n",
      "0  åŒ—äº¬å¸‚  None  None     110000\n",
      "1  ä¸Šæµ·å¸‚  None  None     310000\n",
      "2  å¹¿ä¸œçœ   å¹¿å·å¸‚  None     440100\n",
      "3  å¹¿ä¸œçœ   æ·±åœ³å¸‚  None     440300\n",
      "4  æµ™æ±Ÿçœ   æ­å·å¸‚  None     330100\n"
     ]
    }
   ],
   "source": [
    "# cpcaåäº†å—ï¼Ÿ\n",
    "# å–å‡ºå‡ ä¸ªç¤ºä¾‹åŸå¸‚\n",
    "sample_cities = ['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³', 'æ­å·']\n",
    "\n",
    "sample_province = cpca.transform(sample_cities)\n",
    "print(sample_province)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2114a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OK,æ‰€æœ‰çš„ä¸œè¥¿å‡å·²ç»å®Œæˆï¼Œæˆ‘ä»¬ä¼šæœ€ç»ˆè¾“å‡ºä¸€ä¸ªå®Œæ•´ç‰ˆæœ¬çš„csvæ–‡ä»¶ï¼ŒåŒ…å«äº†ï¼š\n",
    "\"\"\"\n",
    "comment_id è¯„è®ºID\n",
    "score è¯„åˆ†\n",
    "content è¯„è®ºå†…å®¹\n",
    "time è¯„è®ºæ—¶é—´\n",
    "city è¯„è®ºåŸå¸‚\n",
    "gender æ€§åˆ«\n",
    "sentiment æƒ…æ„Ÿå€¾å‘\n",
    "province çœä»½\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "595554da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['comment_id', 'score', 'content', 'time', 'city', 'gender', 'sentiment',\n",
      "       'province'],\n",
      "      dtype='object')\n",
      "\n",
      "ğŸ‰ æ–‡ä»¶å·²ä¿å­˜ä¸º Nezha2_fin.csv\n"
     ]
    }
   ],
   "source": [
    "# reading CSV\n",
    "df = pd.read_csv('C:\\\\Users\\\\é¡¾å­çª\\\\Desktop\\\\ä¸Šçº½å¤§æ–‡ä»¶\\\\2025 SPRING\\\\Nezha2_with_province.csv', encoding='utf-8-sig', low_memory=False)\n",
    "# æ‰“å°columns:\n",
    "print(df.columns)\n",
    "df.to_csv('C:\\\\Users\\\\é¡¾å­çª\\\\Desktop\\\\ä¸Šçº½å¤§æ–‡ä»¶\\\\2025 SPRING\\\\Nezha2_fin.csv', encoding='utf-8-sig', index=False)\n",
    "print(\"\\nğŸ‰ æ–‡ä»¶å·²ä¿å­˜ä¸º Nezha2_fin.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
