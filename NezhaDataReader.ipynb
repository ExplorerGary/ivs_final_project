{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f6137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV的阅读和处理\n",
    "# 导入各种各样的包：\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "# 这是一个数据读取的脚本，如果你需要使用，请务必自己调整文件地址！！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ced1b321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件编码为：GB2312\n"
     ]
    }
   ],
   "source": [
    "with open('C:\\\\Users\\\\顾子琪\\\\Desktop\\\\上纽大文件\\\\2025 SPRING\\\\Nezha2.csv', 'rb') as f:\n",
    "    import chardet\n",
    "    result = chardet.detect(f.read(10000))\n",
    "    encoding = result['encoding']\n",
    "    print(f\"文件编码为：{encoding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b90b3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['comment_id', 'username', 'user_id', 'score', 'content', 'time', 'city',\n",
      "       'gender', 'approve_count', 'reply_count', 'user_level', 'vip_type',\n",
      "       'is_vip', 'avatar_url', 'tags', 'is_purchase', 'is_hot'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\顾子琪\\\\Desktop\\\\上纽大文件\\\\2025 SPRING\\\\Nezha2.csv', encoding='gb18030', low_memory=False)\n",
    "#打印columns:\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5166285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "重构后的数据：\n",
      "   comment_id  score   content             time city gender\n",
      "0  1211665559    5.0   二刷！依旧精彩  2025/4/21 21:16   佛山     未知\n",
      "1  1211652698    5.0  支持一下国产动漫  2025/4/21 21:15   延边      1\n",
      "2  1211623835    5.0   剧情搞笑又紧张  2025/4/21 21:13   武汉     未知\n",
      "3  1211642772    3.5  还行吧，特效不错  2025/4/21 21:08   深圳     未知\n",
      "4  1211659669    4.5    值得看的一部  2025/4/21 20:59   海东     未知\n",
      "\n",
      "✅ 重构后的 CSV 已保存为 'Nezha2_dropped_unnecessary_cols.csv'\n"
     ]
    }
   ],
   "source": [
    "#重构CSV，我们将会丢去一些列：'comment_id', 'username', 'user_id', 'approve_count', 'reply_count', 'user_level', 'vip_type', 'is_vip', 'avatar_url', 'tags', 'is_purchase', 'is_hot'\n",
    "\n",
    "# 要丢弃的列\n",
    "columns_to_drop = [\n",
    "    'username', 'user_id', 'approve_count', 'reply_count',\n",
    "    'user_level', 'vip_type', 'is_vip', 'avatar_url', 'tags',\n",
    "    'is_purchase', 'is_hot'\n",
    "]\n",
    "\n",
    "# 检查这些列是否都存在于当前 DataFrame 中（防止报错）\n",
    "columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "\n",
    "# 丢掉不需要的列\n",
    "df_restructured = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# 查看前几行，确认结构\n",
    "print(\"\\n重构后的数据：\")\n",
    "print(df_restructured.head())\n",
    "\n",
    "# 可选择保存为新文件（以防原文件被覆盖）\n",
    "df_restructured.to_csv('C:\\\\Users\\\\顾子琪\\\\Desktop\\\\上纽大文件\\\\2025 SPRING\\\\Nezha2_dropped_unnecessary_cols.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"\\n✅ 重构后的 CSV 已保存为 'Nezha2_dropped_unnecessary_cols.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "599b5763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['comment_id', 'score', 'content', 'time', 'city', 'gender'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 接下来，我们在新数据集上操作：\n",
    "df = pd.read_csv('C:\\\\Users\\\\顾子琪\\\\Desktop\\\\上纽大文件\\\\2025 SPRING\\\\Nezha2_dropped_unnecessary_cols.csv', encoding='utf-8-sig', low_memory=False)\n",
    "#打印columns:\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f86059f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "正在分析情感: 100%|██████████| 1326/1326 [00:11<00:00, 115.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    content  sentiment\n",
      "0   二刷！依旧精彩   0.797696\n",
      "1  支持一下国产动漫   0.482468\n",
      "2   剧情搞笑又紧张   0.633187\n",
      "3  还行吧，特效不错   0.947526\n",
      "4    值得看的一部   0.852965\n",
      "\n",
      "✅ 情感分析完成并保存为：C:\\Users\\顾子琪\\Desktop\\上纽大文件\\2025 SPRING\\Nezha2_now_with_sentiment.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#情感分析：\n",
    "from snownlp import SnowNLP\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"正在分析情感\")\n",
    "# 检查是否有空值，并填充为 None\n",
    "df['content'] = df['content'].replace('', np.nan)\n",
    "# 使用 SnowNLP 进行情感分析\n",
    "df['sentiment'] = df['content'].progress_apply(lambda x: SnowNLP(str(x)).sentiments if pd.notnull(x) else None)\n",
    "\n",
    "# 显示前几行检查结果\n",
    "print(df[['content', 'sentiment']].head())\n",
    "\n",
    "# 保存带情感分析结果的新 CSV\n",
    "output_path = 'C:\\\\Users\\\\顾子琪\\\\Desktop\\\\上纽大文件\\\\2025 SPRING\\\\Nezha2_now_with_sentiment.csv'\n",
    "df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "print(f\"\\n✅ 情感分析完成并保存为：{output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99056271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a840ac5e670404cadc330f8fdb4e400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\顾子琪\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\顾子琪\\.cache\\huggingface\\hub\\models--IDEA-CCNL--Erlangshen-Roberta-110M-Sentiment. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d80cf5ffa84932ae77ae8277ec1fa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/785 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a47e04a762cc418ba9178c35cc111167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/409M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "情感倾向: tensor([[0.0087, 0.9913]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414faacc20b0436d9b7b6de717046e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/409M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#好吧，这个东西看起来不是特别的准，我们换一个工具包：\n",
    "#GPT： 使用HuggingFace Transformers + 中文 BERT 情感模型\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# 载入模型（示例模型名，可换）\n",
    "model_name = \"IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "text = \"二刷！依旧精彩\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "outputs = model(**inputs)\n",
    "probs = torch.softmax(outputs.logits, dim=1)\n",
    "print(\"情感倾向:\", probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6439c499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Sentiment:   0%|          | 0/1326 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Analyzing Sentiment: 100%|██████████| 1326/1326 [01:48<00:00, 12.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    content  sentiment\n",
      "0   二刷！依旧精彩     0.9913\n",
      "1  支持一下国产动漫     0.9101\n",
      "2   剧情搞笑又紧张     0.9853\n",
      "3  还行吧，特效不错     0.9776\n",
      "4    值得看的一部     0.9496\n",
      "分析完成，文件已保存为： C:\\Users\\顾子琪\\Desktop\\上纽大文件\\2025 SPRING\\Nezha2_sentiment_by_advancedNLPs.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv('C:\\\\Users\\\\顾子琪\\\\Desktop\\\\上纽大文件\\\\2025 SPRING\\\\Nezha2_dropped_unnecessary_cols.csv', encoding='utf-8-sig', low_memory=False)\n",
    "\n",
    "# 加载模型和 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment\")\n",
    "\n",
    "# 放到GPU如果有的话\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 情感分析函数\n",
    "def get_positive_score(text):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return 0.0\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "        positive_score = probs[0][1].item()  # 正面情绪概率\n",
    "    return round(positive_score, 4)\n",
    "\n",
    "# 用 tqdm 处理所有评论\n",
    "tqdm.pandas(desc=\"Analyzing Sentiment\")\n",
    "df[\"sentiment\"] = df[\"content\"].progress_apply(get_positive_score)\n",
    "\n",
    "# 显示前几行检查结果\n",
    "print(df[['content', 'sentiment']].head())\n",
    "\n",
    "# 保存结果\n",
    "output_path = 'C:\\\\Users\\\\顾子琪\\\\Desktop\\\\上纽大文件\\\\2025 SPRING\\\\Nezha2_sentiment_by_advancedNLPs.csv'\n",
    "df.to_csv(output_path, encoding='utf-8-sig', index=False)\n",
    "\n",
    "print(\"分析完成，文件已保存为：\", output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c77e984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 哦对，换了先进的NLP工具包之后，准确度上升了不少。\n",
    "\"\"\"\n",
    "snow NLP说：\n",
    "1211618654\t1.5\t剧情逻辑、人物塑造不能细想\t2025/4/21 10:44\t北京\t1\t0.7763131252505957\n",
    "\n",
    "但是advanced NLP这样说：\n",
    "1211618654\t1.5\t剧情逻辑、人物塑造不能细想\t2025/4/21 10:44\t北京\t1\t0.0027\n",
    "\"\"\"\n",
    "\n",
    "#这就很对了！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd4e2839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 合法省份及评论数量：\n",
      "广东省: 154 条评论\n",
      "江苏省: 144 条评论\n",
      "山东省: 142 条评论\n",
      "北京市: 77 条评论\n",
      "浙江省: 77 条评论\n",
      "广西壮族自治区: 64 条评论\n",
      "山西省: 58 条评论\n",
      "四川省: 52 条评论\n",
      "辽宁省: 48 条评论\n",
      "河南省: 45 条评论\n",
      "湖北省: 39 条评论\n",
      "河北省: 37 条评论\n",
      "陕西省: 36 条评论\n",
      "安徽省: 32 条评论\n",
      "湖南省: 30 条评论\n",
      "内蒙古自治区: 22 条评论\n",
      "云南省: 21 条评论\n",
      "福建省: 20 条评论\n",
      "贵州省: 19 条评论\n",
      "上海市: 18 条评论\n",
      "重庆市: 17 条评论\n",
      "黑龙江省: 17 条评论\n",
      "新疆维吾尔自治区: 16 条评论\n",
      "甘肃省: 14 条评论\n",
      "吉林省: 11 条评论\n",
      "天津市: 11 条评论\n",
      "宁夏回族自治区: 10 条评论\n",
      "江西省: 9 条评论\n",
      "海南省: 9 条评论\n",
      "青海省: 6 条评论\n",
      "西藏自治区: 2 条评论\n",
      "\n",
      "🎉 文件已保存为 Nezha2_with_province.csv\n"
     ]
    }
   ],
   "source": [
    "#OK,接下来，把我们的城市转变为省份：\n",
    "\n",
    "#导包：\n",
    "import pandas as pd\n",
    "import cpca\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv('C:\\\\Users\\\\顾子琪\\\\Desktop\\\\上纽大文件\\\\2025 SPRING\\\\Nezha2_sentiment_by_advancedNLPs.csv', encoding='utf-8-sig', low_memory=False)\n",
    "\n",
    "# 初始化 tqdm 进度条\n",
    "tqdm.pandas(desc=\"解析城市到省份\")\n",
    "\n",
    "# 使用 cpca 时，需要将城市列转换为 pandas Series 类型\n",
    "df['province'] = cpca.transform(df['city'].values)['省']\n",
    "\n",
    "# 填充“未知省份”\n",
    "df['province'] = df['province'].fillna('未知省份')\n",
    "\n",
    "# 打印合法省份及评论数量（去除“未知省份”）\n",
    "province_counts = df[df['province'] != '未知省份']['province'].value_counts()\n",
    "\n",
    "print(\"\\n✅ 合法省份及评论数量：\")\n",
    "for province, count in province_counts.items():\n",
    "    print(f\"{province}: {count} 条评论\")\n",
    "\n",
    "# 保存带省份的 CSV\n",
    "df.to_csv('C:\\\\Users\\\\顾子琪\\\\Desktop\\\\上纽大文件\\\\2025 SPRING\\\\Nezha2_with_province.csv', encoding='utf-8-sig', index=False)\n",
    "\n",
    "print(\"\\n🎉 文件已保存为 Nezha2_with_province.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d97a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     省     市     区 地址  adcode\n",
      "0  北京市  None  None     110000\n",
      "1  上海市  None  None     310000\n",
      "2  广东省   广州市  None     440100\n",
      "3  广东省   深圳市  None     440300\n",
      "4  浙江省   杭州市  None     330100\n"
     ]
    }
   ],
   "source": [
    "# cpca坏了吗？\n",
    "# 取出几个示例城市\n",
    "sample_cities = ['北京', '上海', '广州', '深圳', '杭州']\n",
    "\n",
    "sample_province = cpca.transform(sample_cities)\n",
    "print(sample_province)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2114a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OK,所有的东西均已经完成，我们会最终输出一个完整版本的csv文件，包含了：\n",
    "\"\"\"\n",
    "comment_id 评论ID\n",
    "score 评分\n",
    "content 评论内容\n",
    "time 评论时间\n",
    "city 评论城市\n",
    "gender 性别\n",
    "sentiment 情感倾向\n",
    "province 省份\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "595554da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['comment_id', 'score', 'content', 'time', 'city', 'gender', 'sentiment',\n",
      "       'province'],\n",
      "      dtype='object')\n",
      "\n",
      "🎉 文件已保存为 Nezha2_fin.csv\n"
     ]
    }
   ],
   "source": [
    "# reading CSV\n",
    "df = pd.read_csv('C:\\\\Users\\\\顾子琪\\\\Desktop\\\\上纽大文件\\\\2025 SPRING\\\\Nezha2_with_province.csv', encoding='utf-8-sig', low_memory=False)\n",
    "# 打印columns:\n",
    "print(df.columns)\n",
    "df.to_csv('C:\\\\Users\\\\顾子琪\\\\Desktop\\\\上纽大文件\\\\2025 SPRING\\\\Nezha2_fin.csv', encoding='utf-8-sig', index=False)\n",
    "print(\"\\n🎉 文件已保存为 Nezha2_fin.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
